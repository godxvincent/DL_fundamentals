{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
    "\n",
    "# Team members - Group 66\n",
    "\n",
    "* César Guillermo Vázquez Álvarez - A01197857\n",
    "* Francisco Vázquez Martínez - A01797089\n",
    "* Manuel Alejandro Vázquez Meza - A01796404\n",
    "* Ricardo Andrés Vargas Martínez - A01797243\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_PATH = f'{CURRENT_PATH}/asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''Splits the data into validation and test sets based on the given percentage.\n",
    "    Inputs:\n",
    "        x: numpy array of features\n",
    "        y: numpy array of labels\n",
    "        pct: percentage of data to use for validation\n",
    "        shuffle: whether to shuffle the data before splitting\n",
    "    Outputs:\n",
    "        x_val: numpy array of validation features\n",
    "        y_val: numpy array of validation labels\n",
    "        x_test: numpy array of test features\n",
    "        y_test: numpy array of test labels \n",
    "    '''\n",
    "    if shuffle:\n",
    "        indices = np.arange(x.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "    split_idx = int(x.shape[0] * pct)\n",
    "    x_val = x[:split_idx]\n",
    "    y_val = y[:split_idx]\n",
    "    x_test = x[split_idx:]\n",
    "    y_test = y[split_idx:]\n",
    "    return x_val, y_val, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "### The following\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_mean, x_std, x_data):\n",
    "    \"\"\"\n",
    "    Normalizes the data using the provided mean and standard deviation.\n",
    "    Inputs:\n",
    "        x_mean: mean of the training data\n",
    "        x_std: standard deviation of the training data\n",
    "        x_data: data to be normalised\n",
    "    Outputs:        normalised data \n",
    "    \"\"\"\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "x_train = normalize(x_mean, x_std, x_train)\n",
    "x_val = normalize(x_mean, x_std, x_val)\n",
    "x_test = normalize(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_to_dict(folder_path):\n",
    "    \"\"\"\n",
    "    Loads images from a folder into a dictionary.\n",
    "    Keys: Filename without extension.\n",
    "    Values: Image data (NumPy array).\n",
    "    \"\"\"\n",
    "    image_dict = {}\n",
    "    \n",
    "    # Supported formats for matplotlib\n",
    "    valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: The folder '{folder_path}' does not exist.\")\n",
    "        return image_dict\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            # Get the name without the extension\n",
    "            key = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Create full path and load image\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                img_data = mpimg.imread(img_path)\n",
    "                image_dict[key] = img_data\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load {filename}: {e}\")\n",
    "                \n",
    "    return image_dict\n",
    "\n",
    "\n",
    "def display_two_images(img1,img2, key1, key2):\n",
    "    \"\"\"\n",
    "    Displays two images from the dictionary side by side.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a figure with 1 row and 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "    # Display first image\n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(key1)\n",
    "    axes[0].axis('off')  # Hide tick marks\n",
    "\n",
    "    # Display second image\n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(key2)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The folder './asl_reference' does not exist.\n",
      "Loaded images for letters: []\n",
      "3193\n",
      "Predicted letter: u\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'u'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m predicted_letter = alphabet[y_test[rnd_idx]]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted letter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_letter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m display_two_images(x_test[rnd_idx].reshape(\u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m), \u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_letter\u001b[49m\u001b[43m]\u001b[49m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_letter.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_letter.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'u'"
     ]
    }
   ],
   "source": [
    "images = load_images_to_dict('./asl_reference')\n",
    "print(f\"Loaded images for letters: {list(images.keys())}\")\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "print(rnd_idx)\n",
    "predicted_letter = alphabet[y_test[rnd_idx]]\n",
    "print(f\"Predicted letter: {predicted_letter}\")\n",
    "\n",
    "display_two_images(x_test[rnd_idx].reshape(28, 28), images[predicted_letter], f\"label: {predicted_letter.upper()}\", f\"Reference: {predicted_letter.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuations for our model\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches\n",
    "\n",
    "Mini-batches are small subsets of the total training dataset used to update a model's weights during a single iteration. Instead of calculating the gradient for every single data point at once (Batch Gradient Descent) or for just one sample at a time (Stochastic Gradient Descent), mini-batching strikes a balance that offers the best of both worlds. Its importance lies in computational efficiency—allowing the use of highly optimized matrix operations on GPUs—and regularization, as the inherent noise in small batches helps the model avoid local minima and generalize better. When implementing them, the most critical factor is the batch size: too small, and the training becomes slow and unstable; too large, and you risk exhausting your GPU memory or converging to a \"sharp\" minimum that performs poorly on new data. Additionally, it is vital to shuffle your data every epoch to ensure each batch is a representative, random sample of the overall distribution.\n",
    "\n",
    "We choose to implement a mini-batch loader class vs the simple funtion to create minibatches as its offer some advantages like:\n",
    "- State encapsulation: the class tracks the current position and handles logic like reshuffling at the start of each epoch\n",
    "- Extensibility: You can easily add methods for data augmentation, normalization, or even multi-threading later on.\n",
    "- Lazy loading: loading just one minibatch into memomy instead of the whole dataset* \n",
    "\n",
    "*In this case due to the format of how the data is stored (a single csv file vs individual images) the lazy loading benefits do not apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchLoader:\n",
    "    \"\"\"An iterable mini-batch generator for large-scale data processing.\n",
    "\n",
    "    This class partitions features and labels into smaller chunks (batches). \n",
    "    Unlike static list-based batching, this class uses lazy loading and \n",
    "    index-shuffling to maintain a low memory footprint while ensuring the \n",
    "    stochastic nature of the training process.\n",
    "\n",
    "    Attributes:\n",
    "        x_data (np.ndarray): The input feature matrix of shape (N, ...).\n",
    "        y_data (np.ndarray): The target label array of shape (N, ...).\n",
    "        batch_size (int): The number of samples to return per iteration.\n",
    "        shuffle (bool): If True, re-randomizes sample order every time \n",
    "            the iterator is restarted (at the start of an epoch).\n",
    "        n_samples (int): Total count of samples in the provided dataset.\n",
    "        indices (np.ndarray): Integer array representing the order of samples.\n",
    "        current_idx (int): Internal pointer tracking the progress through \n",
    "            the dataset.\n",
    "\n",
    "    Args:\n",
    "        x_data (np.ndarray): Input features.\n",
    "        y_data (np.ndarray): Target labels.\n",
    "        batch_size (int, optional): Size of each mini-batch. Defaults to 32.\n",
    "        shuffle (bool, optional): Whether to shuffle data per epoch. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_data, y_data, batch_size=32, shuffle=True):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_samples = x_data.shape[0]\n",
    "        self.indices = np.arange(self.n_samples)\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Prepares the loader for a new pass over the data.\n",
    "\n",
    "        If shuffling is enabled, the index array is randomized in-place.\n",
    "        The internal pointer is reset to zero.\n",
    "\n",
    "        Returns:\n",
    "            MiniBatchLoader: The instance itself as an iterator.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        self.current_idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Fetches the next available batch of features and labels.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A pair containing (x_batch, y_batch) as numpy arrays.\n",
    "\n",
    "        Raises:\n",
    "            StopIteration: If all samples have been processed for the current epoch.\n",
    "        \"\"\"\n",
    "        if self.current_idx >= self.n_samples:\n",
    "            raise StopIteration\n",
    "        \n",
    "        start = self.current_idx\n",
    "        end = min(start + self.batch_size, self.n_samples)\n",
    "        \n",
    "        batch_indices = self.indices[start:end]\n",
    "        x_batch = self.x_data[batch_indices]\n",
    "        y_batch = self.y_data[batch_indices]\n",
    "        \n",
    "        self.current_idx = end\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Calculates the total number of batches available.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of batches in one full pass of the data.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Parameter, Linear, ReLU & Sequential Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter class\n",
    "\n",
    "The parameter class acts as a dedicated container for tensors that require gradient tracking during backpropagation. It encapsulates the weight or bias values and their corresponding gradients in a single object.\n",
    "\n",
    "Creatring a class to hold the value and the gradient for the weights or biases has some advantages:\n",
    "- Explicit contract: you now what a parameter object cointains by reading the class.\n",
    "- Type safety: it prevents errors from accidentally triging to access a gradiend before is asigned\n",
    "- Extensibility: additional fucntuonality can be added like a flag to \"Freeze\" layers, add properties like first movement and second movement used un optimized like adam \n",
    "I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Parameter:\n",
    "    \"\"\"\n",
    "    This class encapsulates a weight or bias tensor and its corresponding \n",
    "    gradient. It mimics a standard NumPy array by proxying attribute \n",
    "    access (like .shape, .size, .T) to the underlying data.\n",
    "\n",
    "    Attributes:\n",
    "        value (np.ndarray): The actual numerical data of the parameter.\n",
    "        grad (np.ndarray): The gradient of the loss with respect to this \n",
    "            parameter. Initialized to zeros with the same shape as `value`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = np.array(value)\n",
    "        self.grad = np.zeros_like(self.value)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Redirects attribute access to the underlying NumPy array.\n",
    "\n",
    "        This allows calling `param.shape` or `param.ndim` directly on \n",
    "        the Parameter object instead of `param.value.shape`.\n",
    "        \"\"\"\n",
    "        return getattr(self.value, name)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation including shape and gradient magnitude.\n",
    "        \n",
    "        The gradient norm is a useful debugging metric to check for \n",
    "        vanishing or exploding gradients during training.\n",
    "        \"\"\"\n",
    "        grad_norm = np.linalg.norm(self.grad)\n",
    "        return f\"Parameter(shape={self.shape}, grad_norm={grad_norm:.4e})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter class demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the param:\n",
      " [[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "Gradient of the param:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "String representation of the param:\n",
      " Parameter(shape=(2, 2), grad_norm=0.0000e+00)\n",
      "\n",
      "Accessing the shape of the np.array as it it was an attribute of the param:\n",
      " (2, 2)\n"
     ]
    }
   ],
   "source": [
    "param  = Parameter(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "\n",
    "print(f\"Value of the param:\\n {param.value}\")\n",
    "print(f\"\\nGradient of the param:\\n {param.grad}\")   \n",
    "print(f\"\\nString representation of the param:\\n {param}\")\n",
    "print(f\"\\nAccessing the shape of the np.array as it it was an attribute of the param:\\n {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Class\n",
    "\n",
    "A linear layer acts as the fundamental building block of a neural network, performing an affine transformation on incoming data. It takes an input vector and multiplies it by a weight matrix (W) to project the data into a new feature space, followed by the addition of a bias vector (b). This process allows the network to learn linear relationships between features, effectively scaling, rotating, and shifting the data to identify patterns. While the operation itself is mathematically simple, these layers with non-linear activations allows the network to approximate incredibly complex, high-dimensional functions.\n",
    "\n",
    "We just made an improvemnt on the class shared, by adding a more flexible way to select the initialization method by passing a funtion\n",
    "\n",
    "Forward pass \n",
    "\n",
    " $$y = W^T X + b$$\n",
    "\n",
    "\n",
    "| Term | Mathematical Name | Practical Role |\n",
    "|------|-------------------|----------------|\n",
    "| X    | Input Vector      | The features or data being analyzed. |\n",
    "| W    | Weight Matrix     | The \"learned\" importance of each feature. |\n",
    "| b    | Bias Vector       | The \"flexibility\" to shift the model's baseline. |\n",
    "| y    | Output (Logit)    | The raw prediction or signal for the next layer. |\n",
    "\n",
    "Backward pass \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W} &= X \\frac{\\partial L}{\\partial y} \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial y} \\\\\n",
    "\\frac{\\partial L}{\\partial X} &= W \\frac{\\partial L}{\\partial y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "| Term            | Mathematical Name              | Practical Role |\n",
    "|-----------------|--------------------------------|----------------|\n",
    "| L               | Loss funtion                   | A function measuring how wrong model predictions are compared to targets. |\n",
    "| ∂L/∂y           | Output Gradient                | How much the loss changes with respect to the output. |\n",
    "| ∂L/∂W           | Weight Gradient                | How each weight should change to reduce the loss. |\n",
    "| ∂L/∂b           | Bias Gradient                  | How the bias should shift to reduce the loss. |\n",
    "| ∂L/∂X           | Input Gradient                 | How the input influences the loss (used to propagate error backward). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, init_fn=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Number of input dimensions.\n",
    "            out_features: Number of output dimensions.\n",
    "            init_fn: A callable that takes (out_dim, in_dim) and returns a numpy array.\n",
    "                     If None, defaults to  Kaiming He Initialization.\n",
    "        \"\"\"\n",
    "        if init_fn is None:\n",
    "            limit = np.sqrt(2 / in_features)\n",
    "            weights_val = np.random.randn(out_features, in_features) * limit\n",
    "        else:\n",
    "            weights_val = init_fn(out_features, in_features)\n",
    "            \n",
    "        self.weights = Parameter(weights_val)\n",
    "        self.bias = Parameter(np.zeros((1, out_features)))\n",
    "        self.input_cache = None\n",
    "\n",
    "    def forward(self, x):    \n",
    "        self.input_cache = x\n",
    "        return x @ self.weights.T + self.bias.value\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.weights.grad = grad_output.T @ self.input_cache\n",
    "        self.bias.grad = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        return grad_output @ self.weights.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear class demo\n",
    "\n",
    "We use 2 different ways to initialize the layers, we can notice the sparse initialization leaves a persentange of layers as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_init(out_dim, in_dim, sparsity=0.5):\n",
    "    \"\"\"sparsity: fraction of weights to be non-zero\"\"\"\n",
    "    weights = np.zeros((out_dim, in_dim))\n",
    "    n_nonzero = int(sparsity * in_dim)\n",
    "    \n",
    "    for i in range(out_dim):\n",
    "        indices = np.random.choice(in_dim, n_nonzero, replace=False)\n",
    "        weights[i, indices] = np.random.randn(n_nonzero)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Kaiming He:\n",
      " [[-0.630034   -0.64123431  0.02731634]\n",
      " [-1.33302818 -0.69728019 -0.05039789]]\n",
      "Weights sparse:\n",
      " [[ 0.         -0.32554662  0.        ]\n",
      " [ 0.          0.         -0.96423327]]\n",
      "Bias Kaiming He:\n",
      " [[0. 0.]]\n",
      "Bias sparse:\n",
      " [[0. 0.]]\n",
      "Forward Output Kaiming He: [[0.37693043 0.36957116]]\n",
      "Forward Output sparse: [[ 0.1738824  -0.67476562]]\n",
      "Weight Gradient Shape Kaiming He: (2, 3)\n",
      "Weight Gradient Shape sparse: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1, 3)\n",
    "\n",
    "layer = Linear(3, 2)\n",
    "layer_sparse = Linear(3, 2, sparse_init)\n",
    "\n",
    "# Weights and biases\n",
    "print(f\"Weights Kaiming He:\\n {layer.weights.value}\")\n",
    "print(f\"Weights sparse:\\n {layer_sparse.weights.value}\")\n",
    "print(f\"Bias Kaiming He:\\n {layer.bias.value}\")    \n",
    "print(f\"Bias sparse:\\n {layer_sparse.bias.value}\")    \n",
    "\n",
    "# Forward\n",
    "out = layer.forward(x)\n",
    "print(f\"Forward Output Kaiming He: {out}\")\n",
    "\n",
    "out = layer_sparse.forward(x)\n",
    "print(f\"Forward Output sparse: {out}\")\n",
    "\n",
    "# Backward (dummy gradient of 1s)\n",
    "grad_in = layer.backward(np.ones_like(out))\n",
    "print(f\"Weight Gradient Shape Kaiming He: {layer.weights.grad.shape}\")\n",
    "\n",
    "grad_in = layer_sparse.backward(np.ones_like(out))\n",
    "print(f\"Weight Gradient Shape sparse: {layer_sparse.weights.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU\n",
    "\n",
    "It introduces non-linearity, which is what allows the network to learn complex patterns instead of just acting like one giant linear equation.\n",
    "Mathematically, ReLU is defined as:\n",
    "\n",
    "$$ReLU(x)=max(0,x)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        # Cache to store which elements were > 0 during forward pass\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Elements < 0 become 0.\n",
    "        \"\"\"\n",
    "        self.mask = (x > 0)\n",
    "        return np.where(self.mask, x, 0)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: Gradients are passed back only where x was > 0.\n",
    "        \"\"\"\n",
    "        # The gradient of ReLU is 1 if x > 0, and 0 otherwise.\n",
    "        # We multiply the incoming gradient by this local gradient.\n",
    "        grad_input = grad_output * self.mask\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU demo\n",
    "For the pass forward we notice the negative values are converted to 0 and the positive values pass unchanged\n",
    "\n",
    "For the pass backward we notice the gradients are pass backwards only when the input was greater than 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output: [[0.  0.  1. ]\n",
      " [0.4 2.  0. ]]\n",
      "Gradient Input Shape: (2, 3)\n",
      "Gradient Input:\n",
      " [[0. 0. 1.]\n",
      " [1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "layer_relu = ReLU()\n",
    "x = np.array([[-1,0,1], [.4,2,-1]])\n",
    "\n",
    "# Forward\n",
    "out = layer_relu.forward(x)\n",
    "print(f\"Forward Output: {out}\")\n",
    "\n",
    "# Backward (dummy gradient of 1s)\n",
    "grad_in = layer_relu.backward(np.ones_like(out))\n",
    "print(f\"Gradient Input Shape: {grad_in.shape}\")\n",
    "print(f\"Gradient Input:\\n {grad_in}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential class\n",
    "\n",
    "A Sequential layer acts as the architectural skeleton of a neural network, governing the flow of information through the model's various components. Rather than managing each layer’s operations manually, the Sequential container treats a series of distinct layers as a single, unified pipeline. It ensures that the output of one transformation—whether it be a linear projection or a non-linear activation—is fed seamlessly as the input to the next. By encapsulating this \"forward pass\" logic, the Sequential model simplifies the building of deep architectures, allowing complex hierarchies of features to emerge as data travels through the stack.\n",
    "\n",
    "We removed the responsability of handling the update of the parameters so we can use a external class called optimized this allow to use differnt optimizers and limit the resposability of the sequential class to just pass the values forward and backwards to the secuential or activation layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"\n",
    "        Accepts a variable number of layer instances.\n",
    "        Example: model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Flows data forward through every layer.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Flows gradients backward through every layer in reverse order.\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Utility to collect all Parameter objects (weights/biases) \n",
    "        from all layers for the optimizer.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            # Check if the layer has weights (like Linear)\n",
    "            if hasattr(layer, 'weights'):\n",
    "                params.append(layer.weights)\n",
    "            if hasattr(layer, 'bias'):\n",
    "                params.append(layer.bias)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential class demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward: [[0. 0.]]\n",
      "backward: [[0. 0. 0. 0.]]\n",
      "parameters: [Parameter(shape=(3, 4), grad_norm=0.0000e+00), Parameter(shape=(1, 3), grad_norm=0.0000e+00)]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    Linear(4, 3),\n",
    "    ReLU(),\n",
    "    Linear(3, 2)\n",
    ")\n",
    "x=np.random.randn(1, 4)\n",
    "print(\"forward:\", model.forward(x))\n",
    "print(\"backward:\", model.backward(np.ones((1, 2))))\n",
    "print(\"parameters:\", model.get_parameters()[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "A Cost Function—often referred to as a loss or objective function—serves as the mathematical compass of a neural network, quantifying the discrepancy between the model's predictions and the actual ground truth. By condensing the performance of the entire network into a single scalar value, it provides a concrete measure of \"error\" that the optimization algorithm seeks to minimize.\n",
    "\n",
    "We split the clas in 2 so it allows us to have a class to manage the funtions of the different loss error funtions and another one the loginc to apply them in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunctions:\n",
    "    @staticmethod\n",
    "    def mse(y_pred, y_true):\n",
    "        \"\"\"Mean Squared Error\"\"\"\n",
    "        loss = np.mean((y_pred - y_true)**2)\n",
    "        grad = 2 * (y_pred - y_true) / y_pred.shape[0]\n",
    "        return loss, grad\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y_pred, y_true):\n",
    "        \"\"\"Mean Absolute Error\"\"\"\n",
    "        loss = np.mean(np.abs(y_pred - y_true))\n",
    "        grad = np.sign(y_pred - y_true) / y_pred.shape[0]\n",
    "        return loss, grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_cross_entropy(logits, y_true):\n",
    "        \"\"\"\n",
    "        logits: Raw output from the last Linear layer (unnormalized)\n",
    "        y_true: Ground truth labels (one-hot encoded)\n",
    "        \"\"\"\n",
    "        # 1. Compute Softmax (with numerical stability trick)\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        \n",
    "        # 2. Compute Cross-Entropy Loss\n",
    "        batch_size = logits.shape[0]\n",
    "        # Clipping to avoid log(0)\n",
    "        probs_clipped = np.clip(probs, 1e-12, 1.0)\n",
    "        loss = -np.sum(y_true * np.log(probs_clipped)) / batch_size\n",
    "        \n",
    "        # 3. Compute Gradient: (Probs - True_Labels) / Batch_Size\n",
    "        grad = (probs - y_true) / batch_size\n",
    "        \n",
    "        return loss, grad\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, cost_fn=CostFunctions.mse):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cost_fn: A function that returns (loss_value, gradient)\n",
    "        \"\"\"\n",
    "        self.cost_fn = cost_fn\n",
    "        self.grad = None\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Calculates the loss and stores the gradient for later.\"\"\"\n",
    "        loss_val, self.grad = self.cost_fn(y_pred, y_true)\n",
    "        return loss_val\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Returns the gradient calculated during the forward pass.\"\"\"\n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Value MSE:\n",
      " 1.1705117872255673\n",
      "Gradient of Loss MSE:\n",
      " [[ 2.92986238 -0.88317649]]\n",
      "\n",
      "Loss Value X entropy:\n",
      " 1.245793228929029\n",
      "Gradient of Loss X entropy:\n",
      " [[ 0.7122874 -0.7122874]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    Linear(4, 3),\n",
    "    ReLU(),\n",
    "    Linear(3, 2)\n",
    ")\n",
    "x = np.random.randn(1, 4)\n",
    "y_true = np.array([[0, 1]])\n",
    "y_pred = model.forward(x)\n",
    "\n",
    "loss_mse = Loss(cost_fn=CostFunctions.mse)\n",
    "loss_value = loss_mse.forward(y_pred, y_true)\n",
    "print(f\"Loss Value MSE:\\n {loss_value}\")\n",
    "grad_loss = loss_mse.backward()\n",
    "print(f\"Gradient of Loss MSE:\\n {grad_loss}\")\n",
    "\n",
    "loss_x_entropy = Loss(cost_fn=CostFunctions.softmax_cross_entropy)\n",
    "loss_value = loss_x_entropy.forward(y_pred, y_true)\n",
    "print(f\"\\nLoss Value X entropy:\\n {loss_value}\")\n",
    "grad_loss = loss_x_entropy.backward()\n",
    "print(f\"Gradient of Loss X entropy:\\n {grad_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "It acts as the bridge between static data and a dynamic model, repeatedly exposing the network to batches of information to refine its internal parameters. Within each \"epoch,\" the loop meticulously coordinates three critical phases: the Forward Pass, where data propagates through the layers to generate a prediction; the Backward Pass, where the cost function’s error is signaled back through the architecture via gradients; and the Optimization Step, where the optimizer nudges the weights in a direction that reduces future error. By repeating this cycle thousands of times, the training loop transforms a randomly initialized set of weights into a specialized mathematical tool capable of high-level pattern recognition.\n",
    "\n",
    "We also defined an optimizer to pass the reponsability to ajust the different parameters in this case we used stochastic gradient descent. It updates model parameters using only a single or small subset of training examples per iteration. This approach provides high computational efficiency and faster convergence, making it ideal for large-scale datasets, though it results in a noisy, erratic path towards the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SGD:\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parameters: A list of Parameter objects (weights/biases).\n",
    "            learning_rate: How large of a step to take in the gradient direction.\n",
    "        \"\"\"\n",
    "        self.parameters = parameters\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Updates the value of each parameter: \n",
    "        \n",
    "        Value = Value - (Learning_Rate * Gradient)\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.value -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Resets the gradients of all parameters to zero.\n",
    "        Crucial to call this before every backward pass.\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.grad = np.zeros_like(param.value)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The Sequential model instance.\n",
    "            criterion: The Loss instance (Strategy).\n",
    "            optimizer: The Optimizer instance (e.g., SGD).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y, epochs=100, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the model using the MiniBatchLoader for data iteration.\n",
    "        \"\"\"\n",
    "        # Initialize the data loader\n",
    "        # Note: shuffle=True is standard for training to maintain stochasticity\n",
    "        data_loader = MiniBatchLoader(X, y, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            # Using the MiniBatchLoader as an iterator\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                # 1. Forward Pass\n",
    "                predictions = self.model.forward(X_batch)\n",
    "                loss = self.criterion.forward(predictions, y_batch)\n",
    "                epoch_losses.append(loss)\n",
    "\n",
    "                # 2. Backward Pass\n",
    "                self.optimizer.zero_grad()\n",
    "                grad_loss = self.criterion.backward()\n",
    "                self.model.backward(grad_loss)\n",
    "\n",
    "                # 3. Update Weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            self.loss_history.append(avg_loss)\n",
    "\n",
    "            if verbose and (epoch % (epochs // 10 or 1) == 0 or epoch == epochs - 1):\n",
    "                # We use the last batch of the epoch for the accuracy display\n",
    "                acc = calculate_accuracy(predictions, y_batch)\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {avg_loss:.6f} | Batch Accuracy: {acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional auxialiary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of correct predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model outputs of shape (batch_size, num_classes)\n",
    "        y_true: Ground truth one-hot encoded labels of shape (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    # Convert one-hot or probabilities to class indices (e.g., [0, 0, 1] -> 2)\n",
    "    predicted_classes = np.argmax(y_pred, axis=1)\n",
    "    true_classes = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    # Compare and take the mean\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    return accuracy * 100  # Return as a percentage\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Converts integer labels [3, 1] into [[0,0,0,1], [0,1,0,0]]\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss: 3.831764 | Batch Accuracy: 8.74%\n",
      "Epoch   10 | Loss: 2.205338 | Batch Accuracy: 41.75%\n",
      "Epoch   20 | Loss: 1.658224 | Batch Accuracy: 64.08%\n",
      "Epoch   30 | Loss: 1.320509 | Batch Accuracy: 68.93%\n",
      "Epoch   40 | Loss: 1.087318 | Batch Accuracy: 72.82%\n",
      "Epoch   50 | Loss: 0.919014 | Batch Accuracy: 79.61%\n",
      "Epoch   60 | Loss: 0.788631 | Batch Accuracy: 87.38%\n",
      "Epoch   70 | Loss: 0.686333 | Batch Accuracy: 89.32%\n",
      "Epoch   80 | Loss: 0.606871 | Batch Accuracy: 78.64%\n",
      "Epoch   90 | Loss: 0.533843 | Batch Accuracy: 98.06%\n"
     ]
    }
   ],
   "source": [
    "# Defining the layers of the model\n",
    "model = Sequential(\n",
    "    Linear(784, 400),\n",
    "    ReLU(),\n",
    "    Linear(400, 200),\n",
    "    ReLU(),\n",
    "    Linear(200, 24)\n",
    ")\n",
    "\n",
    "# Defining the loss function and the optimizer\n",
    "criterion = Loss(cost_fn=CostFunctions.softmax_cross_entropy)\n",
    "optimizer = SGD(model.get_parameters(), learning_rate=0.001)\n",
    "\n",
    "#Using all the objects above to build the trainer \n",
    "trainer = Trainer(model, criterion, optimizer)\n",
    "\n",
    "# Train the model\n",
    "y_train_ohe = one_hot_encode(y_train, num_classes=24)\n",
    "trainer.fit(x_train, y_train_ohe, epochs=100, batch_size=526)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.5521472392638\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_ohe = one_hot_encode(y_test, num_classes=24)\n",
    "# Calculating the accuracy of the model \n",
    "y_pred = model.forward(x_test)\n",
    "print(calculate_accuracy(y_pred, y_test_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368\n",
      "Predicted letter: a\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing with a random Image\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "print(rnd_idx)\n",
    "\n",
    "Label = alphabet[y_val[rnd_idx]]\n",
    "predicted_letter = alphabet[np.argmax(model.forward(x_val[rnd_idx].reshape(1, -1)))]\n",
    "print(f\"Predicted letter: {predicted_letter}\")\n",
    "\n",
    "display_two_images(x_val[rnd_idx].reshape(28, 28), images[Label],  f\"Predicted: {predicted_letter.upper()}\", f\"label: {Label.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Completing this exercise highlights that building a neural network is as much an exercise in software engineering as it is in mathematics. By trying to reproduce the code and then refactoring to introduce functional weight initialization, we moved beyond a prototiope script toward a modular framework. This transition to a more maintainable and flexible architecture is a great experience for truly grasping the internal mechanics of a network; it forces you to think about how data flows, how state is managed, and how different components—like the optimizer and the sequential container—must communicate without being tightly coupled.\n",
    "\n",
    "Through this process, we've encountered the core challenges that developers face when implementing performant deep learning systems: balancing memory efficiency (through lazy batching), ensuring numerical stability (through proper initialization), and maintaining extensibility (through decoupled classes). Solving these hurdles manually provides an invaluable perspective on what modern libraries like PyTorch or TensorFlow are doing under the hood. Ultimately, this journey from a simple linear transformation to a robust, iterative training system reveals that the elegance of a neural network lies in its ability to solve complex problems through the repetitive application of simple, well-organized rules.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
